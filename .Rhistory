#checking to see if NA
dt  %>%
summarise_all(list(~is.na(.)))%>%
pivot_longer(everything(),
names_to = "variables", values_to="missing") %>%
count(variables, missing) %>%
filter(missing=="FALSE") %>%
select(variables,n) %>%
rename(Non.Missing=n)
#checking to see if NA
dt  %>%
summarise_all(list(~is.na(.)))%>%
pivot_longer(everything(),
names_to = "variables", values_to="missing") %>%
count(variables, missing) %>%
filter(missing=="FALSE") %>%
select(variables,n) %>%
rename(Non.Missing=n)
#checking to see if NA
dt  %>%
summarise_all(list(~is.na(.)))%>%
pivot_longer(everything(),
names_to = "variables", values_to="missing") %>%
count(variables, missing) %>%
filter(missing=="FALSE") %>%
dplyr::select(variables,n) %>%
rename(Non.Missing=n)
#Bar Charts of Categorical Variables
t<-as.data.frame(table(dt$job))
ggplot(t,aes(x=reorder(Var1,Freq),y=Freq))+
geom_bar(stat = "identity")+
labs(x="Job")+
coord_flip()
t<-as.data.frame(table(dt$marital))
###
ggplot(t,aes(x=reorder(Var1,Freq),y=Freq))+
geom_bar(stat = "identity")+
labs(x="Marital Status")+
coord_flip()
t<-as.data.frame(table(dt$education))
ggplot(t,aes(x=reorder(Var1,Freq),y=Freq))+
geom_bar(stat = "identity")+
labs(x="Education")+
coord_flip()
t<-as.data.frame(table(dt$default))
ggplot(t,aes(x=reorder(Var1,Freq),y=Freq))+
geom_bar(stat = "identity")+
labs(x="Has credit in default?")+
coord_flip()
t<-as.data.frame(table(dt$housing))
ggplot(t,aes(x=reorder(Var1,Freq),y=Freq))+
geom_bar(stat = "identity")+
labs(x="Has housing loan?")+
coord_flip()
t<-as.data.frame(table(dt$loan))
ggplot(t,aes(x=reorder(Var1,Freq),y=Freq))+
geom_bar(stat = "identity")+
labs(x="Has personal loan?")+
coord_flip()
t<-as.data.frame(table(dt$contact))
ggplot(t,aes(x=reorder(Var1,Freq),y=Freq))+
geom_bar(stat = "identity")+
labs(x="Contact Communication Type")+
coord_flip()
t<-as.data.frame(table(dt$month))
ggplot(t,aes(x=reorder(Var1,Freq),y=Freq))+
geom_bar(stat = "identity")+
labs(x="Last contact month of Year")+
coord_flip()
t<-as.data.frame(table(dt$poutcome))
ggplot(t,aes(x=reorder(Var1,Freq),y=Freq))+
geom_bar(stat = "identity")+
labs(x="Previous Marketing Campaign")+
coord_flip()
t<-as.data.frame(table(dt$y))
ggplot(t,aes(x=reorder(Var1,Freq),y=Freq))+
geom_bar(stat = "identity")+
labs(x="Subscribed a Term Deposit?")+
coord_flip()
#Scatterplot Matrix of Numeric Variables
pairs.panels(dt[,c(1,6,10,12:15)])
#Variable Recoding
#Previous Marketing Campaign
dt$poutcome<-ifelse(dt$poutcome=="other","unknown",dt$poutcome)
#Sampling
names(dt[,-c(1,6,10,12:15)])
dt[,-c(1,6,10,12:15)]<-lapply(dt[,-c(1,6,10,12:15)],as.factor)
dt1<-dt
dt1$Index<-c(1:4521)
set.seed(2022)
train <- dt1 %>%
group_by(job,marital,education,default,housing,loan,contact,month,poutcome,y) %>%
sample_frac(size=.70)
test<-dt[-train$Index,]
names(train)
names(test)
train<-train[,-18]
###
#Naive Bayes classifier ####
classifier_cl <- naiveBayes(y ~ ., data =train)
summary(classifier_cl)
# Predicting on train data'
y_pred <- predict(classifier_cl, newdata = test)
# Confusion Matrix
cm <- table(test$y, y_pred)
cm
# Model Evaluation
confusionMatrix(cm)
#### Boosted C5.0#####
fit <- C5.0(y~., data=train, trials=10)
# summarize the fit
print(fit)
# make predictions
predictions <-  predict(fit,test)
# summarize accuracy
tab <- table(predictions, test$y)
confusionMatrix(tab)
####
# Bagging CART######
# load the package
# load data
# fit model
fit <- bagging(y~., data=train)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, test, type="class")
# summarize accuracy
tab <- table(predictions, test$y)
confusionMatrix(tab)
# Dt #########
model<- ctree(y ~ .,train)
plot(model)
# and those who are not
predict_model<-predict(model,test)
# creates a table to count how many are classified
# as native speakers and how many are not
m_at <- table(test$y, predict_model)
m_at
confusionMatrix(m_at)
# LDA model#######
modelda <- lda(y~., data=train)
summary(modelda)
p <- predict(modelda,newdata = test)
p$class
tab <- table(test$y, p$class)
confusionMatrix(tab)
# Random Forest Model#######
# Create a Random Forest model with default parameters
RF <- randomForest(y ~ ., data = train, importance = TRUE)
RF
# Running on Validation Set
model_pred = predict(RF, newdata =test)
table(model_pred)
tab <- table(model_pred, test$y)
confusionMatrix(tab)
#### Cart ####
# load the package
library(rpart)
# fit model
fit <- rpart(y~., data=train)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, test, type="class")
# summarize accuracy
tab <- table(predictions, test$y)
confusionMatrix(tab)
### GBM####
library(caret)
fitControl <- trainControl(method = "cv",
number = 10)
tune_Grid <-  expand.grid(interaction.depth = 2,
n.trees = 500,
shrinkage = 0.1,
n.minobsinnode = 10)
set.seed(825)
fit <- train(y ~ ., data = train,
method = "gbm",
trControl = fitControl,
verbose = FALSE)
predicted= predict(fit,test,type= "prob")[,2]
pred <- ifelse(predicted>=0.5,"yes","no")
table(pred)
tab <- table(test$y,pred)
confusionMatrix(tab)
library(adabag)
model_adaboost <- boosting(y~., data=train, boos=TRUE, mfinal=50)
library(e1071)
classifier = svm(formula = y ~ .,
data = train,
type = 'C-classification',
kernel = 'linear')
library(e1071)
classifier = svm(formula = y ~ .,
data = train,
type = 'C-classification',
kernel = 'linear')
# Predicting the Test set results
y_pred = predict(classifier, newdata = test)
y_pred
tab <- table(y_pred, test$y)
confusionMatrix(tab)
# load class library
library(class)
# Fitting KNN Model
# to training dataset
classifier_knn <- knn(train = train,
test = test,
cl = train$y,
k = 1)
knn(train = train,
test = test,
cl = train$y,
k = 1)
# Fitting KNN Model
# to training dataset
classifier_knn <- knn(train = train,
test = test,
cl = train$y,
k = 5)
# Fitting KNN Model
# to training dataset
classifier_knn <- knn(train = train,
test = test,
cl = test$y,
k = 5)
# Fitting KNN Model
# to training dataset
classifier_knn <- knn(train = train,
test = test,
cl = train$y,
k = 5)
classifier_knn
model = glm(formula=y ~ . , family = binomial(link='logit'),data = train) %>% stepAIC()
summary(model)
probabilities = predict(model,newdata=test,type='response')
prob <- ifelse(probabilities >= 0.5, "yes","no")
table(prob)
tab <- table(prob, test$y)
confusionMatrix(tab)
setwd("D:/Naimul Vai work/Charlesbook project")
setwd("D:/Naimul Vai work/Charlesbook project")
dim(training);
setwd("D:/Naimul Vai work/Charlesbook project")
data <- read.csv("CharlesBookClubs.csv")
setwd("D:/Naimul Vai work/Charlesbook project")
data <- read.csv("CharlesBookClubs.csv")
data <- read.csv("CharlesBookClub.csv")
set.seed(1)
intrain <- createDataPartition(y = data$Florence, p= 0.6, list = FALSE)
training <- data[intrain,]
testing <- data[-intrain,]
View(testing)
dim(training);
dim(testing);
anyNA(data)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(1)
knn_fit <- train(Florence ~., data = training, method = "knn",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
knn_fit
str(data)
summary(data)
data[,-c(2,3,4,5)]<-lapply(data[,-c(2,3,4,5)],as.factor)
str(data)
table(data$ChildBks)
set.seed(1)
intrain <- createDataPartition(y = data$Florence, p= 0.6, list = FALSE)
training <- data[intrain,]
testing <- data[-intrain,]
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(1)
knn_fit <- train(Florence ~., data = training, method = "knn",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
knn_fit
test_pred <- predict(knn_fit, newdata = testing)
test_pred
confusionMatrix(test_pred, testing$Florence )
table(data$Florence)
table(training$Florence)
table(testing$Florence)
table(test_pred, testing$Florence)
cl_tree_fit <- train(Florence ~., data = training,
method = "rpart",
trControl = trainControl(method = "cv"))
cl_tree_fit
# plot the model
plot(cl_tree_fit$finalModel, uniform=TRUE,
main="Classification Tree")
plot(cl_tree_fit$finalModel, uniform=TRUE,
main="Classification Tree")
library(rpart.plot)
# plot the model
plot(cl_tree_fit$finalModel, uniform=TRUE,
main="Classification Tree")
suppressMessages(library(rattle))
install.packages("rattle")
library(rattle)
fancyRpartPlot(cl_tree_fit$finalModel)
cl_tree_fit$finalModel
plot(cl_tree_fit)
summary(cl_tree_fit$finalModel)
dt_test_pred <- predict(cl_tree_fit, newdata = testing)
dt_test_pred
confusionMatrix(dt_test_pred, testing$Florence )
confusionMatrix(dt_test_pred, testing$Florence )
fitControl
set.seed(1)
fit <- train(Floarance~ ., data = training,
method = "gbm",
trControl = trctrl,
verbose = FALSE)
set.seed(1)
fit <- train(Florence~ ., data = training,
method = "gbm",
trControl = trctrl,
verbose = FALSE)
fit <- train(Florence~ ., data = training,
method = "gbm",
trControl = trctrl,
verbose = FALSE)
set.seed(1)
fit <- train(Florence~ ., data = training,
method = "gbm",
trControl = trctrl,
verbose = FALSE)
gbm_predicted= predict(fit,testing,type= "prob")[,2]
pred <- ifelse(gbm_predicted>=0.3,"1","0")
table(pred)
pred <- ifelse(gbm_predicted>=0.5,"1","0")
table(pred)
gbm_predicted
gbm_predicted= predict(fit,testing,type= "prob")[,2]
pred <- ifelse(gbm_predicted>=0.5,"1","0")
table(pred)
pred <- ifelse(gbm_predicted <=0.5,"0","1")
table(pred)
pred
gbm_predicted= predict(fit,testing,type= "prob")[,2]
gbm_predicted <=0.5
c <- gbm_predicted <=0.5
d <- gbm_predicted > 0.5
d
tab <- table(training$Florence,pred)
tab <- table(testing$Florence,pred)
confusionMatrix(tab)
table(testing$Florence,pred)
#Metric compare model is Accuracy
metric <- "Accuracy"
set.seed(1)
#Number randomely variable selected is mtry
mtry <- sqrt(ncol(x))
#Metric compare model is Accuracy
metric <- "Accuracy"
set.seed(1)
rf_default <- train(Florence~.,
data=training,
method='rf',
metric='Accuracy',
trControl=trctrl)
rf_default <- train(Florence~.,
data=training,
method='rf',
metric='Accuracy')
default_glm_mod = train(
form = Florence ~ .,
data =training,
method = "glm",
family = "binomial"
)
probabilities = predict(default_glm_mod,newdata=testing,type='response')
probabilities = predict(default_glm_mod,newdata=testing)
probabilities = predict(default_glm_mod,testing)
probabilities = predict(default_glm_mod,testing, type = "probs")
default_glm_mod
default_glm_mod$results
default_glm_mod$finalModel
calc_acc = function(actual, predicted) {
mean(actual == predicted)
}
calc_acc
head(predict(default_glm_mod, newdata = testing))
# test acc
calc_acc(actual = testing$Florence,
predicted = predict(default_glm_mod, newdata = testing$Florence))
actual = testing$Florence
predicted = predict(default_glm_mod, newdata = testing$Florence
predicted = predict(default_glm_mod, newdata = testing$Florence)
calc_acc(actual = testing$Florence,
predicted = predict(default_glm_mod, newdata = testing$Florence))
head(predict(default_glm_mod, newdata = testing))
pred_glm <- predict(default_glm_mod, newdata = testing)
table(pred_glm)
confusionMatrix(pred_glm, testing$Florence)
head(pred_glm)
default_glm_mod
pred_glm <- predict(default_glm_mod, newdata = testing)
head(pred_glm)
table(pred_glm)
confusionMatrix(pred_glm, testing$Florence)
rf_default <- train(Florence~.,
data=training,
method='rf')
setwd("D:/Naimul Vai work/Charlesbook project")
data <- read.csv("CharlesBookClub.csv")
dim(training);
dim(testing);
anyNA(data)
summary(data)
str(data)
data[,-c(2,3,4,5)]<-lapply(data[,-c(2,3,4,5)],as.factor)
set.seed(1)
set.seed(1)
intrain <- createDataPartition(y = data$Florence, p= 0.6, list = FALSE)
library(caret)
### Data Slicing
set.seed(1)
intrain <- createDataPartition(y = data$Florence, p= 0.6, list = FALSE)
training <- data[intrain,]
testing <- data[-intrain,]
rf_default <- train(Florence~.,
data=training,
method='rf')
RF <- randomForest(Florence ~ ., data = training, importance = TRUE)
library(randomForest)
RF <- randomForest(Florence ~ ., data = training, importance = TRUE)
RF
# Running on Validation Set
model_pred = predict(RF, newdata =testing)
table(model_pred)
confusionMatrix(model_pred,testing$Florence)
setwd("D:/Naimul Vai work/Charlesbook project")
library(caret)
data <- read.csv("CharlesBookClub.csv")
##### Set working directory #####
setwd("D:/Naimul Vai work/Charlesbook project")
### required library
library(caret)
### Load the data set
data <- read.csv("CharlesBookClub.csv")
#For checking the dimensions of our training data frame and validation data frame, we can use these:
dim(training);
anyNA(data)
summary(data)
str(data)
data[,-c(2,3,4,5)]<-lapply(data[,-c(2,3,4,5)],as.factor)
set.seed(1)
set.seed(1)
intrain <- createDataPartition(y = data$Florence, p= 0.6, list = FALSE)
training <- data[intrain,]
validation <- data[-intrain,]
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(1)
knn_fit <- train(Florence ~., data = training, method = "knn",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
knn_fit
test_pred <- predict(knn_fit, newdata = validation)
test_pred
knn_cm <-  confusionMatrix(test_pred, validation$Florence )
knn_cm
cl_tree_fit <- train(Florence ~., data = training,
method = "rpart",
trControl = trainControl(method = "cv"))
cl_tree_fit
dt_test_pred <- predict(cl_tree_fit, newdata = validation)
dt_test_pred
classification_tree <-  confusionMatrix(dt_test_pred, validation$Florence )
classification_tree
default_glm_mod = train(
form = Florence ~ .,
data =training,
method = "glm",
family = "binomial"
)
default_glm_mod
default_glm_mod$results
default_glm_mod$finalModel
pred_glm <- predict(default_glm_mod, newdata = validation)
head(pred_glm)
table(pred_glm)
confusionMatrix(pred_glm, validation$Florence)
rf_default <- train(Florence~.,
data=training,
method='rf')
library(randomForest)
RF <- randomForest(Florence ~ ., data = training, importance = TRUE)
RF
# Running on Validation Set
model_pred = predict(RF, newdata =validation)
logistic_cm <- confusionMatrix(pred_glm, validation$Florence)
# Running on Validation Set
model_pred = predict(RF, newdata =validation)
random_cm <- confusionMatrix(model_pred,validation$Florence)
logistic_cm
random_cm
library(Metrics)
accuracy(random_cm)
accuracy(model_pred,validation$Florence)
accuracy(test_pred,validation$Florence)
accuracy(model_pred,validation$Florence)
accuracy(pred_glm,validation$Florence)
accuracy(model_pred,validation$Florence)
accuracy(pred_glm,validation$Florence)
accuracy(dt_test_pred,validation$Florence)
accuracy(test_pred,validation$Florence)
